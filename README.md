# LRRIT-LLM
**Learning Response Review and Improvement Tool – LLM Agentic Prototype**

This repository contains a prototype **agentic framework** for applying Large Language Models (LLMs) to the *Learning Response Review and Improvement Tool (LRRIT)* used in healthcare safety governance.

The system ingests incident / learning response documents (PDFs), extracts structured evidence, and evaluates them across multiple LRRIT dimensions using **dimension-specific agents**, with outputs designed to be auditable, explainable, and suitable for human comparison.

---

## Purpose

The project explores whether LLMs can:

- Apply LRRIT dimensions consistently
- Provide **evidence-anchored judgements** (verbatim quotes)
- Distinguish between:
  - systems vs individual framing,
  - learning actions vs analysis,
  - blame vs non-blame language
- Support **human review**, not replace it

The idea is to have:

- Clean separation of dimensions (D1–D8) with no conceptual leakage
- Evidence-grounded outputs (verbatim, auditable, polarity-aware)
- Appropriate uncertainty handling (especially in D5–D7)
- An agentic architecture that actually reflects the LRRIT rubric, not a superficial mapping
- A system that can be explained to clinicians, safety scientists, and auditors.

Further explanation can be found below.

NB. This codebase is intended for **research, prototyping, and governance evaluation**, not operational deployment.

---

## High-level architecture

1. **Ingest**
   - Extracts text and tables from PDF reports
2. **EvidencePack**
   - Normalises extracted content into an auditable structure
3. **Dimension-specific agents**
   - Each agent evaluates one LRRIT dimension only
   - Agents return structured JSON with ratings, rationale, evidence quotes, and uncertainty
   - Each agent is assessed for its performance by the LLM (LLM-as-Judge or LaJ), using a basket of metrics for task evaluation to reduce the risk of hallucination, errors, misalignment etc. by the agents.
     - *NB. The LaJ does not assess the agents against the report, it assesses them against the rubric and the verbatim evidence retrieved by the agent.*
4. **Presentation**
   - Results rendered as static HTML for human review
   - Dynamic drop down allows user to drill down into detail for evaluation metrics.

---
<img width="1644" height="878" alt="image" src="https://github.com/user-attachments/assets/5c65d686-c6bd-4512-95fa-ebb446be8152" />

## Repository structure

```text
lrrit-llm/
│
├── src/
│   └── lrrit_llm/
│       ├── agents/                       # Dimension-specific agents
│           ├── profiles/                 # Agent profiles in mark down (reference only)
│       │      └── *.md                      # Agent design notes
│       │   ├── d1_compassion.py          # D1: Compassionate engagement
│       │   ├── d2_systems.py             # D2: Systems approach
│       │   ├── d3_learning_actions.py    # D3: Human error / learning actions
│       │   ├── d4_blame.py               # D4: Blame language avoided
│       │   ├── d5_local_rationality.py   # D5: Local rationality / reasoning
│       │   ├── d6_counterfactuals.py     # D6: Counterfactual reasoning
│       │   ├── d7_actions.py             # D7: Safety Actions / recs to take
│       │   ├── d8_clarity.py             # D8: Communication quality and usability
│       │
│       ├── ingest/
│       │   ├── pdf_text.py         # Text extraction (PyMuPDF)
│       │   └── pdf_tables.py       # Table extraction
│       │
│       ├── evidence/
│       │   ├── schema.py           # EvidencePack data model
│       │   └── pack.py             # EvidencePack builder / serializer
│       │ 
│       ├── laj/
│       │   ├── laj_meta.py           # LLM-as_Judge metrics and rules
│       │   ├── dimensions_defs.py    # summary of data dimensions
│       │
│       └── clients/
│           └── openai_client.py    # LLM client wrapper
│
├── scripts/
│   ├── test_agents.py              # Test script to run agents against a hardcoded pdf report
│   └── render_results_html.py      # Render agent results to HTML
│
├── data/
│   ├── raw_pdfs/                   # Input PDF reports
│   └── processed/
│       └── reports/
│           └── <report_id>/
│               ├── evidence_pack.json
│               ├── agent_results.json
│               ├── laj_results.json        # LaJ QA over D1–D8
│               └── agent_results.html # Generated by render_results.py
│
├── README.md
└── requirements.txt
```

---

## Key concepts

### EvidencePack
A structured, auditable container holding:

- text chunks (with provenance)
- extracted tables (when present)
- stable hashes for traceability

All agents operate **only** on the EvidencePack.   

## PDF parsing and evidence extraction

PDF reports are ingested using open-source libraries (`PyMuPDF` for text extraction and `pdfplumber` for tables). We do this so that the text extraction from the reports is not LLM dependent, otherwise what is extracted might change each time we change the main model. By using standard python libraries, we can better guarantee consistency. However, the performance of these libraries on real world data leaves much to be desired, and we may need to find an alternative mechanism.

Text is extracted page-by-page and normalised into traceable text chunks, while tables (where present) are extracted separately and preserved with fallback textual representations. All extracted content is wrapped in an **EvidencePack** with explicit provenance (source file, page number, extractor), ensuring that every agent judgement can be traced back to the original document.


### Dimension-specific agents
Each agent:

- evaluates **one LRRIT dimension only**
- returns **strict JSON** (machine-parseable)
- cites **verbatim evidence quotes** (auditable)
- flags uncertainty explicitly

Implemented data dimension agents:

| Agent | Dimension |
|---|---|
| D1 | Compassionate engagement |
| D2 | Systems approach to contributory factors |
| D3 | Quality & appropriateness of learning actions |
| D4 | Blame language avoided |
| D5 | Local rationality (why actions made sense at the time)|
| D6 | Counterfactuals: How outcomes & alternatives are reasoned about afterwards|
| D7 | Safety actions / recommendations|
| D8 | Communication quality and usability of the learning response|

---

### Evidence polarity
Each evidence item is labelled as:

- `positive` → supports the dimension judgement
- `negative` → weakens/contradicts the dimension judgement (or indicates absence/over-reliance on individual action depending on dimension)

This supports transparent human review and later LLM-as-Judge (LaJ) consistency checking. 

The LRRIT guidance words on evidence - GOOD, SOME, LITTLE - are used to grade the dimensions with supporting rationale. 

The `uncertainty` flag is used to flag the need for human review. It can be triggered differently according to the data dimensions. In cases where no verbatim quotes support the grading, the uncertainty flag is usually set to 'YES'.

---

## LaJ evaluation metrics
The LLM-as-judge or LaJ asesses how well the agents have performed their task. It tries to see if the evidence they cite and their rationale could be erroneous, hallucinated or incoherent / misaligned. 

*It does not judge the agent outputs against the report in question.*

Below are a basket of metrics that are currently unweighted. However, we may weight them to give us a combined single grade as a confidence metric. 

### LaJ metric basket
1. Rubric Fidelity
   - Does the rationale address the intended LRRIT judgement criteria for the dimension?
2. Evidence Grounding
   - Are claims in the rationale supported by the cited excerpts?
3. Reasoning Quality & Internal Coherence
   - Does the rationale logically support the rating without generic/circular statements?
4. Values Alignment (PSIRF/LRRIT)
   - Does the rationale reflect PSIRF/LRRIT values (systems thinking, compassion, local rationality, avoid blame/counterfactual misuse)?
5. Transparency & Uncertainty Handling
   - Is uncertainty signalled appropriately for mixed/ambiguous evidence?
6. Hallucination Screening (agent-output level)
   - Does the rationale introduce claims not supported by the supplied excerpts?
  
The LaJ grades each of these metrics as follows
- PASS: clearly meets the metric
- WARN: partially meets; minor gaps
- FAIL: materially fails; unreliable

It assigns an overall grade to the agent's performance based on the same grades. However, this may change if we introduce weights into the metrics (especially no. 2 and 6).

## Installation

Python **3.10+** recommended.

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## Configuration

Set your OpenAI API key (PowerShell example):

```powershell
setx OPENAI_API_KEY "sk-..."
```

Optional model override:

```powershell
setx OPENAI_MODEL "gpt-4o-mini"
```

---

## Running the pipeline

### 1) Add a PDF
Place a PDF in:

```text
data/raw_pdfs/test.pdf
```

The filename (without extension) becomes the report ID (`test`).

### 2) Run agents 

From repository root:

```powershell
py .\scripts\test_agents.py
```

Outputs will be saved to:

```text
data/processed/reports/test/
```

### 3) Render HTML report

```powershell
py .\scripts\render_results_html.py
```
A full example can be found here: https://raw.githack.com/amlas-tool/lrrit-llm/main/data/processed/reports/test/agent_results.html

<img width="1505" height="1839" alt="image" src="https://github.com/user-attachments/assets/91287a40-661e-4c43-aeca-5a7ff5b189d4" />




---

## Outputs

| File | Purpose |
|---|---|
| `evidence_pack.json` | Auditable extracted evidence |
| `agent_results.json` | Structured agent outputs |
| `laj_results.json`   | Structured LaJ evals for each agent |
| `agent_results.html` | Human-readable report containing agent outputs and LaJ evaluations (generated by render_results_html.py)|

---


## Status

- ✔ EvidencePack ingestion stable (text + optional tables)
- ✔ D1–D8 agents implemented and calibrated
- ✔ HTML presentation layer
- ✔ LLM-as-Judge (LaJ) meta-evaluation layer
- ⏳ Feature extraction for complexity analysis (planned)
- ⏳ Human–LLM comparison tooling (planned)

---

## Disclaimer

This project is a **research prototype**.
It must **not** be used for operational safety governance without formal validation, clinical oversight, and organisational approval.
